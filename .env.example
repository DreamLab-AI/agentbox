# Agentbox Environment Configuration
# Copy to .env and fill in your values

# =============================================================================
# API Keys (Required for full functionality)
# =============================================================================

# Anthropic Claude API Key (required for Z.AI service)
ANTHROPIC_API_KEY=

# GitHub Personal Access Token (for GitHub integration skills)
GITHUB_TOKEN=

# =============================================================================
# Optional API Keys (for specific skills)
# =============================================================================

# Google Gemini API Key (for Gemini integration)
GOOGLE_GEMINI_API_KEY=

# OpenAI API Key (for OpenAI-compatible services)
# For cloud API mode: set to your OpenAI key
# For local Ollama mode: set to "ollama"
OPENAI_API_KEY=

# OpenAI Base URL (for local Ollama inference)
# Uncomment for local Ollama mode:
# OPENAI_BASE_URL=http://host.docker.internal:11434/v1

# OpenRouter API Key (for model routing)
OPENROUTER_API_KEY=

# Brave Search API Key (for web search)
BRAVE_API_KEY=

# =============================================================================
# Management API Configuration
# =============================================================================

# Management API port (default: 9090)
MANAGEMENT_API_PORT=9090

# Management API authentication key
MANAGEMENT_API_KEY=change-this-secret-key

# JWT secret for token signing
JWT_SECRET=change-this-jwt-secret

# CORS allowed origins (comma-separated)
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080

# =============================================================================
# Z.AI Service Configuration
# =============================================================================

# Z.AI port (default: 9600, internal only)
ZAI_PORT=9600

# Z.AI worker pool size
CLAUDE_WORKER_POOL_SIZE=4

# Maximum queue size
CLAUDE_MAX_QUEUE_SIZE=100

# =============================================================================
# MCP Configuration
# =============================================================================

# MCP TCP port
MCP_TCP_PORT=9500

# MCP WebSocket port
MCP_WS_PORT=3002

# MCP log level (debug, info, warn, error)
MCP_LOG_LEVEL=info

# Claude Flow max sessions
CLAUDE_FLOW_MAX_SESSIONS=10

# =============================================================================
# RuVector Standalone Vector Database
# Rust-native with embedded redb storage - NO PostgreSQL required
# Features: HNSW indexing (150x-12,500x faster), GNN layers, self-learning
# =============================================================================

# RuVector data directory (embedded redb storage)
RUVECTOR_DATA_DIR=/var/lib/ruvector

# RuVector API port
RUVECTOR_PORT=9700

# RuVector log level (debug, info, warn, error)
RUVECTOR_LOG_LEVEL=info

# Vector embedding model (all-MiniLM-L6-v2 = 384 dimensions)
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSIONS=384

# =============================================================================
# Security & Rate Limiting
# =============================================================================

# Enable rate limiting (true/false)
RATE_LIMIT_ENABLED=true

# Rate limit window in milliseconds
RATE_LIMIT_WINDOW_MS=60000

# Maximum requests per window
RATE_LIMIT_MAX_REQUESTS=100

# Auto-block suspicious requests
AUTO_BLOCK_ENABLED=false

# =============================================================================
# Logging
# =============================================================================

# Log level (debug, info, warn, error)
LOG_LEVEL=info

# Log directory
LOG_DIR=/var/log

# =============================================================================
# Runtime Configuration
# =============================================================================

# Node.js environment
NODE_ENV=production

# Workspace directory
WORKSPACE=/workspace

# Maximum request size
MAX_REQUEST_SIZE=50mb

# =============================================================================
# VNC Remote Desktop (Desktop Image Only)
# Access via SSH tunnel: ssh -L 5901:localhost:5901 user@host
# =============================================================================

# Enable VNC on startup (default: false for headless)
ENABLE_VNC=false

# VNC display number
VNC_DISPLAY=:1

# VNC resolution
VNC_RESOLUTION=1920x1080x24

# VNC port (localhost only, use SSH tunnel)
VNC_PORT=5901

# =============================================================================
# Local LLM via Ollama (AMD GPU / Local Inference Mode)
# See README.md "Local GPU Mode" section for setup instructions
# =============================================================================

# Ollama API endpoint
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# Default model for local inference
# Recommended: qwen2.5:32b-instruct (Q4_K_M, ~19GB, fits 32GB VRAM)
# OLLAMA_MODEL=qwen2.5:32b-instruct
