# Agentic QE Multi-Model Router Configuration
# Custom providers for Turbo Flow Claude environment
# Version: 2.8.2

# Provider Definitions
providers:
  # Z.AI - Cost-effective Claude wrapper (internal service)
  zai:
    type: openai-compatible
    baseUrl: http://localhost:9600
    defaultModel: glm-4.7
    apiKeyEnv: ZAI_ANTHROPIC_API_KEY
    supportsStreaming: true
    supportsCaching: false
    supportsEmbeddings: false
    supportsVision: false
    costPer1MTokens:
      input: 0.50   # ~80% cheaper than direct Anthropic
      output: 1.50
    limits:
      requestsPerMinute: 60
      tokensPerMinute: 100000
    tags: [cost-effective, internal, claude-compatible]

  # OpenAI - GPT models
  openai:
    type: openai
    baseUrl: https://api.openai.com/v1
    defaultModel: gpt-4o
    apiKeyEnv: OPENAI_API_KEY
    supportsStreaming: true
    supportsCaching: false
    supportsEmbeddings: true
    supportsVision: true
    costPer1MTokens:
      input: 2.50
      output: 10.00
    limits:
      requestsPerMinute: 500
      tokensPerMinute: 200000
    models:
      gpt-4o:
        costPer1MTokens: { input: 2.50, output: 10.00 }
        contextWindow: 128000
        capabilities: [chat, vision, function-calling]
      gpt-4o-mini:
        costPer1MTokens: { input: 0.15, output: 0.60 }
        contextWindow: 128000
        capabilities: [chat, vision, function-calling]
      gpt-4-turbo:
        costPer1MTokens: { input: 10.00, output: 30.00 }
        contextWindow: 128000
        capabilities: [chat, vision, function-calling]
      gpt-3.5-turbo:
        costPer1MTokens: { input: 0.50, output: 1.50 }
        contextWindow: 16385
        capabilities: [chat, function-calling]
    tags: [premium, vision, embeddings]

  # DeepSeek - Cost-effective reasoning models
  deepseek:
    type: openai-compatible
    baseUrl: https://api.deepseek.com/v1
    defaultModel: deepseek-chat
    apiKeyEnv: DEEPSEEK_API_KEY
    supportsStreaming: true
    supportsCaching: true
    supportsEmbeddings: false
    supportsVision: false
    costPer1MTokens:
      input: 0.14   # Very cost effective
      output: 0.28
    limits:
      requestsPerMinute: 60
      tokensPerMinute: 60000
    models:
      deepseek-chat:
        costPer1MTokens: { input: 0.14, output: 0.28 }
        contextWindow: 64000
        capabilities: [chat, code, reasoning]
      deepseek-reasoner:
        costPer1MTokens: { input: 0.55, output: 2.19 }
        contextWindow: 64000
        capabilities: [chat, code, deep-reasoning, chain-of-thought]
    tags: [cost-effective, reasoning, code]

  # Google Gemini (enhanced config)
  gemini:
    type: google
    baseUrl: https://generativelanguage.googleapis.com/v1beta
    defaultModel: gemini-2.0-flash
    apiKeyEnv: GOOGLE_GEMINI_API_KEY
    supportsStreaming: true
    supportsCaching: false
    supportsEmbeddings: true
    supportsVision: true
    costPer1MTokens:
      input: 0.075  # Flash pricing
      output: 0.30
    limits:
      requestsPerMinute: 60
      tokensPerMinute: 1000000
    models:
      gemini-2.0-flash:
        costPer1MTokens: { input: 0.075, output: 0.30 }
        contextWindow: 1048576
        capabilities: [chat, vision, code, multimodal]
      gemini-2.0-flash-thinking:
        costPer1MTokens: { input: 0.075, output: 0.30 }
        contextWindow: 32767
        capabilities: [chat, reasoning, chain-of-thought]
      gemini-1.5-pro:
        costPer1MTokens: { input: 1.25, output: 5.00 }
        contextWindow: 2097152
        capabilities: [chat, vision, code, multimodal, long-context]
    tags: [vision, multimodal, long-context, cost-effective]

# Model Router Configuration
router:
  # Default routing strategy
  strategy: cost-optimized  # Options: cost-optimized, quality-first, balanced, latency-first

  # Task-based routing rules
  routing_rules:
    # Simple tasks -> cheapest models
    simple:
      priority: [zai, deepseek, gemini, openai]
      models:
        - provider: zai
          model: glm-4.7
        - provider: deepseek
          model: deepseek-chat
        - provider: gemini
          model: gemini-2.0-flash
        - provider: openai
          model: gpt-4o-mini
      maxCostPer1MTokens: 1.0

    # Code generation -> reasoning models
    code:
      priority: [deepseek, openai, gemini, zai]
      models:
        - provider: deepseek
          model: deepseek-chat
        - provider: openai
          model: gpt-4o
        - provider: gemini
          model: gemini-2.0-flash
      maxCostPer1MTokens: 5.0

    # Complex reasoning -> best reasoners
    reasoning:
      priority: [deepseek, openai, gemini]
      models:
        - provider: deepseek
          model: deepseek-reasoner
        - provider: openai
          model: gpt-4o
        - provider: gemini
          model: gemini-2.0-flash-thinking
      maxCostPer1MTokens: 10.0

    # Vision tasks -> vision-capable models
    vision:
      priority: [gemini, openai]
      models:
        - provider: gemini
          model: gemini-2.0-flash
        - provider: openai
          model: gpt-4o
      requiresCapability: vision

    # Test generation -> balanced cost/quality
    test_generation:
      priority: [deepseek, zai, gemini, openai]
      models:
        - provider: deepseek
          model: deepseek-chat
        - provider: zai
          model: glm-4.7
        - provider: gemini
          model: gemini-2.0-flash
        - provider: openai
          model: gpt-4o-mini
      maxCostPer1MTokens: 2.0

    # Security analysis -> thorough models
    security:
      priority: [openai, deepseek, gemini]
      models:
        - provider: openai
          model: gpt-4o
        - provider: deepseek
          model: deepseek-reasoner
        - provider: gemini
          model: gemini-1.5-pro
      maxCostPer1MTokens: 15.0

  # Fallback configuration
  fallback:
    enabled: true
    maxRetries: 3
    retryDelay: 1000  # ms
    fallbackChain: [zai, deepseek, gemini, openai]

  # Cost tracking
  costTracking:
    enabled: true
    budgetPerDay: 50.00  # USD
    alertThreshold: 0.80  # 80% of budget
    logUsage: true

# Health check configuration
healthChecks:
  enabled: true
  interval: 60000  # 1 minute
  timeout: 5000    # 5 seconds
  endpoints:
    zai: http://localhost:9600/health
    openai: https://api.openai.com/v1/models
    deepseek: https://api.deepseek.com/v1/models
    gemini: https://generativelanguage.googleapis.com/v1beta/models
